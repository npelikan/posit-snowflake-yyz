---
title: "Running Models in Snowflake"
---

## Data Ingestion

Please run the following steps in your Snowflake Account _once_.

```{sql}
CREATE DATABASE IF NOT EXISTS LENDING_CLUB;
CREATE SCHEMA IF NOT EXISTS ML;

CREATE STAGE LENDING_CLUB.ML.LOAN_DATA
  URL = 's3://posit-snowflake-mlops';

CREATE OR REPLACE FILE FORMAT LENDING_CLUB.ML.FORMAT_LENDING_CLUB_PARQUET
  TYPE = PARQUET
  NULL_IF = ('NULL', 'null');

CREATE OR REPLACE TABLE LENDING_CLUB.ML.LOAN_DATA
  USING TEMPLATE (
    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))
    FROM TABLE(
      INFER_SCHEMA(
        LOCATION => '@LENDING_CLUB.ML.LOAN_DATA/loan_data.parquet',
        FILE_FORMAT => 'LENDING_CLUB.ML.FORMAT_LENDING_CLUB_PARQUET' -- Fully qualified name required
      )
    )
  );

COPY INTO LENDING_CLUB.ML.LOAN_DATA
  FROM '@LENDING_CLUB.ML.LOAN_DATA/loan_data.parquet'
  FILE_FORMAT = (FORMAT_NAME = LENDING_CLUB.ML.FORMAT_LENDING_CLUB_PARQUET)
  MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;

```

## Connect to Snowflake

<!-- 
IMPORTANT -- if you're running this for the first time, you may run into namespacing errors in Connect or permissioning errors in Snowflake. That means someone has already run this with the same `model_name` -- to run this flawlessly, pick a new model_name below.
-->

```{r setup}
#| echo: false

library(odbc)
library(DBI)
library(dbplyr)
library(tidyverse)
library(glue)

con <- dbConnect(
  odbc::snowflake(),
  warehouse = "DEFAULT_WH",
  database = "LENDING_CLUB",
  schema = "ML"
)

model_name <- "r-interest-rate"
```

## Load a sample of the data.

```{r}
lendingclub_dat <- con |> tbl("LOAN_DATA") |>
  mutate(ISSUE_YEAR = as.integer(str_sub(ISSUE_D, start = 5)),
         ISSUE_MONTH = as.integer(case_match(
           str_sub(ISSUE_D, end = 3),
          "Jan" ~ 1,
          "Feb" ~ 2,
          "Mar" ~ 3,
          "Apr" ~ 4,
          "May" ~ 5,
          "Jun" ~ 6,
          "Jul" ~ 7,
          "Aug" ~ 8,
          "Sep" ~ 9,
          "Oct" ~ 10,
          "Nov" ~ 11,
          "Dec" ~ 12
           ))
         )

lendingclub_sample <- lendingclub_dat |>
  filter(ISSUE_YEAR == 2016) |>
  slice_sample(n = 5000)
```

## Only select columns of interest and download the data locally

```{r}
lendingclub_prep <- lendingclub_sample |> 
  select(INT_RATE, TERM, BC_UTIL, BC_OPEN_TO_BUY, ALL_UTIL) |> 
  mutate(
    INT_RATE = as.numeric(stringr::str_remove(INT_RATE, "%"))
    ) |>
  filter(!if_any(everything(), is.na)) |>
  collect()
```

## Create model using `tidymodels`

1.  Run the following code to create a `workflow` that contains the pre-processing steps, and a linear regression model

```{r}
library(tidymodels)

lendingclub_rec <- recipe(INT_RATE ~ ., data = lendingclub_prep) |>
  step_mutate(TERM = (TERM == "60 months")) |>
  # step_dummy(TERM) |>
  # step_mutate(TERM = trimws(substr(TERM, 1,4))) |> 
  step_mutate(across(!TERM, as.numeric)) |> 
  step_normalize(all_numeric_predictors()) |>
  step_impute_mean(all_of(c("BC_OPEN_TO_BUY", "BC_UTIL"))) |>   
  step_filter(!if_any(everything(), is.na))


lendingclub_lr <- linear_reg()

lendingclub_wf <- workflow() |> 
  add_model(lendingclub_lr) |> 
  add_recipe(lendingclub_rec)

lendingclub_wf
```

2.  Fit the model in the workflow, now in a variable called `lendingclub_wf`, with the `lendingclub_prep` data, and compute summary metrics

```{r}
lendingclub_fit <- lendingclub_wf |> 
  fit(data = lendingclub_prep)

lendingclub_metric_set <- metric_set(rmse, mae, rsq)

lendingclub_metrics <- lendingclub_fit |>
  augment(lendingclub_prep) |>
  lendingclub_metric_set(truth = INT_RATE, estimate = .pred)

lendingclub_metrics
```

4.  Register the model using `vetiver`

```{r}
library(vetiver)
library(pins)
library(arrow)

board <- board_connect()
v <- vetiver_model(lendingclub_fit, model_name, metadata = list(metrics = lendingclub_metrics))
board |> vetiver_pin_write(v)

model_versions <- board |>
  pin_versions(glue("{board$account}/{model_name}"))

model_versions
```

Let's also save the model version number -- we're going to need it later!

```{r}
model_version <- model_versions |>
  filter(active) |>
  pull(version)
```

## Convert to an Orbital Object

This allows us to run predictions in Snowflake

```{r}
library(orbital)
library(tidypredict)

orbital_obj <- orbital(lendingclub_fit)
orbital_obj

```

3.  Measure the performance of the model using `vetiver_compute_metrics()`, and plot performance by month

```{r}
lendingclub_16 <- lendingclub_dat |>
  filter(ISSUE_YEAR == 2016)

preds <- orbital_obj |> 
  predict(lendingclub_16) |>
  collect()

augmented_table <- lendingclub_16 |>
  select(ISSUE_MONTH, ISSUE_YEAR, INT_RATE) |>
  collect() |>
  bind_cols(preds) |>
  mutate(date = ymd(glue("{ISSUE_YEAR}-{str_pad(ISSUE_MONTH, 2, pad=0)}-01"))) |>
  arrange(date)

original_metrics <- augmented_table |> 
  vetiver_compute_metrics(date, "month", INT_RATE, .pred)

# pin the metrics
pin_write(board, original_metrics, glue("{model_name}_metrics"), type = "arrow")

original_metrics |>
  vetiver_plot_metrics()
```

```{r}
orbital_sql(orbital_obj, con)
```

## Send Model to Snowflake, and save predictions in a temporary table

```{r}
preds <- predict(orbital_obj, lendingclub_dat) |>
  compute(name="INTEREST_RATE_PREDICTIONS_TEMP")

preds |>
  filter(!is.na(.pred))
```

How many predictions just happened?

```{r}
preds |> count()
```

```{r}
sql_predictor <- orbital_sql(orbital_obj, con)
sql_predictor
```

## Real-time inference

But what if, rather than batch inference, we want to do realtime inference? We can easily do that via deploying the model as an API to Posit Connect!

```{r}
# vetiver_deploy_rsconnect(board, glue("{board$account}/{model_name}"))
```

## Deploy as a Snowflake View

Why use a View? Views are only run when they're called, and can be made part of a Snowflake Optimized Query plan. This will often make them more efficient than Stored Procedures.

```{r}
## Add predictions column to table
res <- dplyr::mutate(lendingclub_dat, !!!orbital_inline(orbital_obj))

# select only the prediction column and the ID column
pred_name <- ".pred"
res <- dplyr::select(res, dplyr::any_of(c("ID", pred_name)))

# Translate the dbplyr `tbl` into a sql query string
generated_sql <- remote_query(res)
```

Translate the generated SQL into a Snowflake View. We'll start by creating a 'versioned' view, linked to the version of the model we just fit.

```{r}
versioned_view_name <- glue("{model_name}_v{model_version}")
snowflake_view_statement <- glue::glue_sql(
  "CREATE OR REPLACE VIEW {`versioned_view_name`} AS ",
  generated_sql,
  .con = con
)

```

Dispatch our view creation statement to Snowflake

```{r}
con |>
  DBI::dbExecute(snowflake_view_statement)
```

We'll next create a 'main' view, which we'll keep updated to match the latest version of the model. This will allow downstream projects to always reference this view, and get the latest updates.

```{r}
main_view_name <- glue("{model_name}_latest")
main_view_statement <- glue::glue_sql(
  "CREATE OR REPLACE VIEW {`main_view_name`} AS ",
  "SELECT * FROM {`versioned_view_name`}",
  .con = con
)

con |>
  DBI::dbExecute(main_view_statement)

```

Now, let's try our view!

```{r}
con |>
  tbl(main_view_name) |>
  head(500) |>
  collect()
```
